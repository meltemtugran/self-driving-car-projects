{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "self driving car final (cop)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meltemtugran/self-driving-car-projects/blob/main/self_driving_car_final_(cop).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5zXViC449-i"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkrpbTbt5d7j"
      },
      "source": [
        "# Step1: Read from the log file\n",
        "samples = []\n",
        "with open('driving_log.csv') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    next(reader, None)\n",
        "    for line in reader:\n",
        "        samples.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs_egWXf5hQk"
      },
      "source": [
        "# Step2: Divide the data into training set and validation set\n",
        "train_len = int(0.8*len(samples))\n",
        "valid_len = len(samples) - train_len\n",
        "train_samples, validation_samples = data.random_split(samples, lengths=[train_len, valid_len])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2vx_K105ntX"
      },
      "source": [
        "# Step3a: Define the augmentation, transformation processes, parameters and dataset for dataloader\n",
        "def augment(imgName, angle):\n",
        "  name = 'data/IMG/' + imgName.split('/')[-1]\n",
        "  current_image = cv2.imread(name)\n",
        "  current_image = current_image[65:-25, :, :]\n",
        "  if np.random.rand() < 0.5:\n",
        "    current_image = cv2.flip(current_image, 1)\n",
        "    angle = angle * -1.0  \n",
        "  return current_image, angle\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, samples, transform=None):\n",
        "\n",
        "        self.samples = samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      \n",
        "        batch_samples = self.samples[index]\n",
        "        \n",
        "        steering_angle = float(batch_samples[3])\n",
        "        \n",
        "        center_img, steering_angle_center = augment(batch_samples[0], steering_angle)\n",
        "        left_img, steering_angle_left = augment(batch_samples[1], steering_angle + 0.4)\n",
        "        right_img, steering_angle_right = augment(batch_samples[2], steering_angle - 0.4)\n",
        "\n",
        "        center_img = self.transform(center_img)\n",
        "        left_img = self.transform(left_img)\n",
        "        right_img = self.transform(right_img)\n",
        "\n",
        "        return (center_img, steering_angle_center), (left_img, steering_angle_left), (right_img, steering_angle_right)\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_yPyRJy5tmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1d82c7-c160-4736-dadd-4f15c83aa401"
      },
      "source": [
        "# Step3b: Creating generator using the dataloader to parallasize the process\n",
        "transformations = transforms.Compose([transforms.Lambda(lambda x: (x / 255.0) - 0.5)])\n",
        "\n",
        "params = {'batch_size': 32,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 4}\n",
        "\n",
        "training_set = Dataset(train_samples, transformations)\n",
        "training_generator = data.DataLoader(training_set, **params)\n",
        "\n",
        "validation_set = Dataset(validation_samples, transformations)\n",
        "validation_generator = data.DataLoader(validation_set, **params)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x_z12F95zdI"
      },
      "source": [
        "# Step4: Define the network\n",
        "class NetworkDense(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NetworkDense, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 24, 5, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(24, 36, 5, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(36, 48, 5, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(48, 64, 3),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(64, 64, 3),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=64 * 2 * 33, out_features=100),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(in_features=100, out_features=50),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(in_features=50, out_features=10),\n",
        "            nn.Linear(in_features=10, out_features=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, input):  \n",
        "        input = input.view(input.size(0), 3, 70, 320)\n",
        "        output = self.conv_layers(input)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.linear_layers(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class NetworkLight(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NetworkLight, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 24, 3, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(24, 48, 3, stride=2),\n",
        "            nn.MaxPool2d(4, stride=4),\n",
        "            nn.Dropout(p=0.25)\n",
        "        )\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=48*4*19, out_features=50),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(in_features=50, out_features=10),\n",
        "            nn.Linear(in_features=10, out_features=1)\n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.view(input.size(0), 3, 70, 320)\n",
        "        output = self.conv_layers(input)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.linear_layers(output)\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdAzK04Y57op"
      },
      "source": [
        "# Step5: Define optimizer\n",
        "model = NetworkLight()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "criterion = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbPuVxr65_dR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca6049f-3346-4e91-d8e6-f9c1be52fea0"
      },
      "source": [
        "# Step6: Check the device and define function to move tensors to that device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "print('device is: ', device)\n",
        "\n",
        "def toDevice(datas, device):\n",
        "  \n",
        "  imgs, angles = datas\n",
        "  return imgs.float().to(device), angles.float().to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device is:  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vydW0tRI6Cqd"
      },
      "source": [
        "# Step7: Train and validate network based on maximum epochs defined\n",
        "max_epochs = 22\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    \n",
        "    model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBr12_My6G0Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "e0947b0f-d64c-4ed2-febf-69b70bf93b3b"
      },
      "source": [
        "# Training\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    for local_batch, (centers, lefts, rights) in enumerate(training_generator):\n",
        "        # Transfer to GPU\n",
        "        centers, lefts, rights = toDevice(centers, device), toDevice(lefts, device), toDevice(rights, device)\n",
        "        \n",
        "        # Model computations\n",
        "        optimizer.zero_grad()\n",
        "        datas = [centers, lefts, rights]        \n",
        "        for data in datas:\n",
        "            imgs, angles = data\n",
        "#             print(\"training image: \", imgs.shape)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, angles.unsqueeze(1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.data[0].item()\n",
        "            \n",
        "        if local_batch % 100 == 0:\n",
        "            print('Loss: %.3f '\n",
        "                 % (train_loss/(local_batch+1)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-d966a44d606b>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    model.train()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efMbLOHQ6Vm-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}